/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pytorch_lightning/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse"
Global seed set to 23
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name                 | Type               | Params
------------------------------------------------------------
0 | resnet               | ResNet             | 14.4 M
1 | fc1                  | Linear             | 15.4 K
2 | fc2                  | Linear             | 7.7 K
3 | fc3                  | Linear             | 33
4 | train_macro_accuracy | MulticlassAccuracy | 0
5 | val_macro_accuracy   | MulticlassAccuracy | 0
6 | test_macro_accuracy  | MulticlassAccuracy | 0
7 | train_accuracy       | MulticlassAccuracy | 0
8 | val_accuracy         | MulticlassAccuracy | 0
9 | test_accuracy        | MulticlassAccuracy | 0
------------------------------------------------------------
14.4 M    Trainable params
0         Non-trainable params
14.4 M    Total params
57.764    Total estimated model params size (MB)
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  category=PossibleUserWarning,
Train dataset length: 10
Validation dataset length: 4

Sanity Checking DataLoader 0:   0%|                                                                                                         | 0/2 [00:00<?, ?it/s]
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.

  "Trying to infer the `batch_size` from an ambiguous collection. The batch size we"
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  category=PossibleUserWarning,
Epoch 0:   0%|                                                                                                                              | 0/7 [00:00<?, ?it/s]
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")